# hello, i'm archisa!
Thanks for showing interest in me and my work!
Reach out to me at archisa28@gmail.com :)
---------------------------------------------------------------------------------------------------------------

## ğŸª¨ğŸ“„âœ‚ï¸ _-_- play a round! :: Rock Â· Paper Â· Scissors ğŸª¨ğŸ“„âœ‚ï¸_-_-

Choose your move:

- ğŸª¨ [Rock](https://github.com/archipelagoing/archipelagoing/issues/new?title=RPS:+Rock&body=click+create+/+leave+a+hidden+message)
- ğŸ“„ [Paper](https://github.com/archipelagoing/archipelagoing/issues/new?title=RPS:+Paper&body=click+create+/+leave+a+hidden+message)
- âœ‚ï¸ [Scissors](https://github.com/archipelagoing/archipelagoing/issues/new?title=RPS:+Scissors&body=click+create+/+leave+a+hidden+message)

<!-- BEGIN RPS STATUS -->
**Rounds played:** 3  
**Last result:** You chose Rock. Computer chose Rock. Tie.
<!-- END RPS STATUS -->

ğŸª¨ğŸ“„âœ‚ï¸ -_-_(This runs via GitHub Actions â€” results appear after a short delay.)_-_- ğŸª¨ğŸ“„âœ‚ï¸


---------------------------------------------------------------------------------------------------------------
# ğŸ‘©ğŸ½â€ğŸ’» A couple of my ongoing projects! ğŸ‘©ğŸ½â€ğŸ’»
## Front Burner
- **Colab Lux**  
  A Chrome/Colab workflow tool for auto-generating lightweight docs (timestamp + summary) and pushing updates to Git (e.g., via Gist), with optional â€œrefactor/ticketâ€ prompts.
    - Chrome/Colab-integrated workflow tool -  for documenting work as it happens
    - **Auto-generates lightweight documentation** (timestamps, short summaries, code context)
    - **Pushes documentation directly to GitHub** (e.g., via Gists) to create a traceable dev history
    - Designed to **minimize context-switching** between coding, thinking, and writing 
    - Exploring **minimal ticketing** and refactor prompts to track iteration effort without heavy PM tools

- **SituatiONION (Transformer Architecture + Mid-Layers Visualization)**  
  A layerwise study of GPT-2 XL focusing on how representations evolve across depth, with clean visualizations of mid-layer â€œsituation modelingâ€ and layer-to-layer dynamics.
  - Layerwise analysis of transformer models (focused on GPT-2 XL)
  - Treats early, mid, and late layers as functionally distinct regimes
  - Focuses on mid-layers where higher-level abstractions and relational structure emerge
  - Builds clean visualizations of representation flow across layers
  - Frames internal model behavior geometrically (manifold / flow-based) rather than token-only
  - Aims to make transformer internals legible to humans, not just measurable via probes
 
## Back Burner
- **Reasoning Engine (WIP)**  
  A structured reasoning layer for agents (human or LLM) that outputs a readable trace + overview/TOC, designed to balance clarity with constraint and avoid common failure modes (collapse/ramble).
  
  - Experimental reasoning layer designed as an explicit intermediate representation
  - Produces structured, human-readable reasoning traces instead of post-hoc explanations
  - Outputs both high-level overviews (e.g., TOC / outline) and fine-grained steps
  - Informed by common LLM failure modes (premature collapse, rambling, over-constraint)
  - Intended for both human decision support and agentic LLM systems
  - Explores how constraint and flexibility can be balanced to stabilize reasoning

- **Chrome Tab Manager (Agentic)**  
  - Agent-assisted Chrome extension for managing complex browsing workflows
  - Clusters tabs by task, theme, and temporal context
  - Generates summaries to preserve intent across sessions
  - Reduces cognitive overload during research-heavy or multi-threaded work
  - Emphasizes user control with lightweight automation rather than full autonomy

----------------
## About Me
- I study how decisions are madeâ€”by people and by machines.
- Iâ€™m interested in the structure behind good outcomes, not just the outcomes themselves.
- Transformer models gave me a concrete way to examine decision-making from an external, inspectable system.
- My work focuses on internal representations, mid-layer reasoning, and interpretability.
- Iâ€™ve spent the past several months analyzing GPT-2 XL to understand how meaning emerges and stabilizes across layers.

